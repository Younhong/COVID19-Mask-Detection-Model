{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1조 인공지능을 활용한 마스크 착용 감지 모델 (고윤홍, 박신, 안혜빈)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트를 하게 된 계기\n",
    "\n",
    " - 전국적으로 확진이 다시 늘어나고 있는 추세이고 마스크 미착용자에 대한 처벌이 강화되고 있습니다. 하지만 마스크를 올바른 방법으로 착용한 사람들 사이에 그렇지 않은 사람들이 존재합니다. 그런 이들을 통해 COVID19의 감염이 전파되는 것을 예방하고 마스크 착용에 대한 감시역이 없더라도 마스크 착용을 권장하는 인공지능을 만들어보고 싶어서 이 프로젝트를 하게 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트 개요\n",
    "\n",
    " - 기존의 마스크 착용 감지 인공지능을 이용하여 실시간 영상 속에서 인원 수를 화면에 표시하고 각각의 마스크 착용 비율을 표시하며 마스크를 착용하지 않은 사람이 섞여있는 경우를 탐지할 수 있는 인공지능을 목표로 하고 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공지능 학습 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫번째 단계: 모듈 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **사용된 모듈**\n",
    "```pyrthon\n",
    " - keras             # tensorflow 위에서 작동하며 더 상위 수준의 기능을 제공\n",
    " - sklearn           # machine learning의 분석에서 중요\n",
    " - imutils           # Open CV가 처리하기 힘든 이미지나 비디오 스트림 파일의 처리를 보완\n",
    " - matplotlib        # 파이썬에서 그래프 표시\n",
    " - numpy             # 저 수준의 언어로 파이썬의 리스트보다 효율적\n",
    " - argparse          # 다양한 인자 관리\n",
    " - os                # 컴퓨터의 디렉토리, 파일을 이용가능하게 만드는 기능\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 두번째 단계: 외부 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-d\", \"--dataset\", required=True, \n",
    "                help=\"path to input dataset\")                    # --dataset 데이터 불러오기\n",
    "ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\", \n",
    "                help=\"path to output loss/accuracy plot\")        # --plot의 데이터를 불러와 plot.png에 저장\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, default=\"mask_detector.model\", \n",
    "                help=\"path to output face mask detector model\")  # --model의 데이터를 불러와 mask_detector.model에 저장\n",
    "args = vars(ap.parse_args())                                     # args에 각 데이터들을 업데이트\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 세번째 단계: 인공지능 학습에 필요한 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "INIT_LR = 1e-4     # 학습률: 0.0001\n",
    "EPOCHS = 20        # 에폭\n",
    "BS = 32            # 배치 크기\n",
    "\n",
    "print(\"[INFO] loading images...\") \n",
    "imagePaths = list(paths.list_images(args[\"dataset\"]))      # dataset의 이미지 데이터들을 배열로 만들기\n",
    "data = []          # 문제의 보기와 같은 역할을 할 배열\n",
    "labels = []        # 문제의 정답과 같은 역할을 할 배열\n",
    "\n",
    "for imagePath in imagePaths:\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "    image = load_img(imagePath, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "# 각각의 배열을 넘파이 배열로 만들기\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels) \n",
    "\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels) \n",
    "\n",
    "# data와 lables배열을 나누기\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, \n",
    "                                                  test_size=0.20, stratify=labels, random_state=42) \n",
    "\n",
    "# 이미지 데이터 전처리 과정\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 네번째 단계:  모델 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#모바일 환경에서 작동가능하도록 최적화\n",
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다섯번째 단계: 학습 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 모델 가져오기\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# 학습\n",
    "print(\"[INFO] training head...\")\n",
    "H = model.fit(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    validation_data=(testX, testY),\n",
    "\tvalidation_steps=len(testX) // BS,\n",
    "    epochs=EPOCHS)\n",
    "\n",
    "# 예측값 구하기 및 출력\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=BS)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs, target_names=lb.classes_))\n",
    "\n",
    "# mask_detector.model에 저장\n",
    "print(\"[INFO] saving mask detector model...\")\n",
    "model.save(args[\"model\"], save_format=\"h5\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여섯번째 단계: 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# plot.png에 저장\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(args[\"plot\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영상을 통한 데이터 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.video import VideoStream\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Argument setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ap = argparse.ArgumentParser()   #  --를 인식함.    외부 데이터 불러들이기\n",
    "\n",
    "ap.add_argument(\"-f\", -face\"-\", type=str,   #외부에서  --face, --model, 그리고 --confidence의 데이터를 받아옴.\n",
    "    default=\"face_detector\",\n",
    "    help=\"path to face detector model directory\")    #--face의 데이터는 face_detector에 추가합니다.\n",
    "    \n",
    "ap.add_argument(\"-m\", \"--model\", type=str,\n",
    "    default=\"mask_detector.model\",\n",
    "    help=\"path to trained face mask detector model\")      #--model의 데이터는 mask_detector.model에 추가합니다.\n",
    "    \n",
    "ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
    "    help=\"minimum probability to filter weak detections\")    \n",
    "    \n",
    "args = vars(ap.parse_args())   #마지막으로 args에 각각의 데이터들을 업데이트해줍니다\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) detect_and_predict_mask 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def detect_and_predict_mask(frame, faceNet, maskNet):\n",
    "    (h, w) = frame.shape[:2]   #frame배열의 1,2번째 index값을 h(세로),w(가로)에 저장   #frame은 실시간 비디오의 화면\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),(104.0, 177.0, 123.0))  #이미지 가공해서 blob에 저장\n",
    "\n",
    "    faceNet.setInput(blob)  #blob에 저장된 가공한 이미지를 faceNet에 저장\n",
    "    detections = faceNet.forward()  #순방향으로 딥러닝 네트워크를 실행해 detection에 저장\n",
    "\n",
    "    faces = []  #얼굴을 모으는 리스트\n",
    "    locs = []   #box의 좌표를 모으는 리스트\n",
    "    preds = []  #마스크 예측한 것을 모으는 리스트\n",
    "    \n",
    "    for i in range(0, detections.shape[2]):  #200번 반복\n",
    "        confidence = detections[0, 0, i, 2]  #얼굴을 인식하고 마스크를 착용했는지에 대한 추측 정확도를 confidence에 저장\n",
    "                                              #detections는 실시간으로 찍히는 화면이 저장되는 것\n",
    "\n",
    "        if confidence > args[\"confidence\"]:   #기본값이 0.5로 설정. confidence가 크면 마스크를 잘 쓴 것.\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h]   #detection에서 얼굴크기를 이용해 box의 네 모퉁이를 box에 저장.\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")  #box의 두 모퉁이 값을 정수로 바꿔서 좌표로 지정\n",
    "            (startX, startY) = (max(0, startX), max(0, startY))   #좌표가 음수가 안나오게 하기 위함.\n",
    "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))   #box가 화면 밖으로 안나가게 하기 위함.\n",
    "\n",
    "            face = frame[startY:endY, startX:endX]   #frame(배경+사람)에서 box크기 만클을 slicing 한 것을 face에 저장.\n",
    "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "            face = cv2.resize(face, (224, 224))   \n",
    "            face = img_to_array(face)\n",
    "            face = preprocess_input(face)\n",
    "\n",
    "            faces.append(face)    #가공된 face가 faces에 추가됨.\n",
    "            locs.append((startX, startY, endX, endY))   #locs에는 box의 좌표가 추가됨.\n",
    "\n",
    "    if len(faces) > 0:   #저장된 얼굴의 개수가 1개 이상이면 얼굴 사진들을 넘파이 어레이로 만듦.\n",
    "        faces = np.array(faces, dtype=\"float32\")\n",
    "        preds = maskNet.predict(faces, batch_size=32)   #예측한 것(마스크를 착용했을 확률과 착용하지 않았을 확률)  \n",
    "\n",
    "    return  (locs,preds)  #box의 좌표들과 예측한 값을 return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) face_detector폴더 내의 파일들을 불러와 faceNet에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print(\"[INFO] loading face detector model...\")\n",
    "\n",
    "prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])   #\"deploy.prototxt\"파일을 불러와서 prototxtPath에 저장 \n",
    "\n",
    "weightsPath = os.path.sep.join([args[\"face\"],\"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "\n",
    "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)  #dnn(딥러닝 네트워크)를 사용해 prototxtPath와 weightPath를 faceNet에 저장\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) mask_detector.model을 불러와 maskNet에 저장, 비디오 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "print(\"[INFO] loading face mask detector model...\")\n",
    "maskNet = load_model(args[\"model\"])      #mask_detector.model을 불러와서 model에 저장하고, 그 model을 maskNet이라는 변수에 저장.\n",
    "\n",
    "print(\"[INFO] starting video stream...\")\n",
    "vs = VideoStream(src=0).start()     #video stream을 초기화하고, 카메라 센서를 warm up시킴.\n",
    "time.sleep(2.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) 실시간 video를 읽어오고, q키를 누르면 video stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "while True:\n",
    "\n",
    "    frame = vs.read()  #실시간 화면이 frame\n",
    "    frame = imutils.resize(frame, width=1100)  #frame크기 조정\n",
    "\n",
    "    (locs, preds) = detect_and_predict_mask(frame, faceNet, maskNet)   #frame(화면)과 4,5번에서 얻은 faceNet,maskNet을 함수에 통과시킴.\n",
    "\n",
    "    maskCount = 0     #추가한 부분\n",
    "    nonMaskCount = 0  #추가한 부분    #실시간으로 사람 수를 세기 위해 매번 초기화함. \n",
    "    for (box, pred) in zip(locs, preds):    #box의 좌표들이 있는 locs변수와 예측한 것이 모여있는 pred변수에서 각각 하나씩 세트로 보냄. \n",
    "        (startX, startY, endX, endY) = box   #box의 2점의 좌표\n",
    "        (mask, withoutMask) = pred   #마스크를 착용했을 확률, 착용하지 않았을 확률을 pred에 저장\n",
    "\n",
    "        if mask > withoutMask: #추가한 부분  #반복문에 들어온 사람이 마스크를 착용했을 확률이 더 높다면 \n",
    "            maskCount += 1     #추가한 부분\n",
    "        else:                  #추가한 부분\n",
    "            nonMaskCount += 1  #추가한 부분  #이렇게 매 순간 마스크를 착용한 사람과 착용하지 않을 사람의 수를 헤아린다. \n",
    "\n",
    "        label = \"Mask\" if mask > withoutMask else \"No Mask\"  #마스크를 착용했을 확률이 높다면 label에 Mask, 아니면 No Mask\n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)   #전자의 경우라면 초록색으로, 후자라면 빨간색으로 글자를 표시한다.\n",
    "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100) \n",
    "        label2 = \"Good Job\" if mask > withoutMask else \"Warning[Wear Mask]\"  #추가한 부분  #착용 여부에 대한 반응을 해줌.\n",
    "        \n",
    "        cv2.putText(frame, label, (startX, startY - 10)   #frame에 위의 label꾸며서 보이게 함.,                           \n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)     \n",
    "        cv2.putText(frame, label2, (startX, endY + 20),   #추가한 부분  #frame에 위의 label2를 꾸며서 출력\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)      \n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY),color, 2)   #frame에 꾸민 box를 올림.\n",
    "\n",
    "    color2 = (0, 255, 0) if nonMaskCount == 0 else (0, 0, 255)    #추가한 부분  #한 명이라도 마스크를 착용하지 않았으면 빨간색으로 color지정\n",
    "    label3 = \"{}/{}\".format(maskCount, nonMaskCount + maskCount)  #추가한 부분  #몇 명 중, 몇 명이 마스크를 착용했는지에 대한 확률 계산\n",
    "    cv2.putText(frame, label3, (100, 130),cv2.FONT_HERSHEY_SIMPLEX, 0.45, color2, 2)  #추가한 부분  #frame위에 color2색을 이용해 label3을 보여줌.\n",
    "\n",
    "    label4 = \"# of Wearing Masks/# of Detected\"   #추가한 부분\n",
    "    cv2.putText(frame, label4, (100, 100),        #추가한 부분\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 0, 0), 2)   #frame위에 label4를 보여줌.\n",
    "    \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF   #사용자가 종료할 때 까지 기다림.\n",
    "\n",
    "    if key == ord(\"q\"):\n",
    "        break   #사용자가 'q'키를 누르면 실시간 비디오 종료된. \n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "vs.stop()   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데모 가이드 문서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 프로젝트 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 명령어를 실행하여 프로젝트를 다운로드한다.\n",
    "```\n",
    "$ git clone https://github.com/younhong/COVID19-Mask-Detection-Model\n",
    "```\n",
    "\n",
    "\n",
    "> 만약 git이 설치되어 있지 않다면 [링크](https://github.com/younhong/COVID19-Mask-Detection-Model)로 들어가서 수동으로 zip 파일을 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경설정: 가상환경을 설정   \n",
    "Why?\n",
    "> 가상환경을 설치하지 않으면 기존의 파이썬 설치된 환경과 충돌하여 의도치 않은 에러가 날 수 있다.\n",
    "\n",
    "##### Step1) 가상환경 생성에 필요한 패키지 설치\n",
    "Mac의 경우\n",
    "```\n",
    "$ pip install virtualenv virtualenvwrapper\n",
    "```\n",
    "\n",
    "Window의 경우\n",
    "```\n",
    "$ pip install virtualenv virtualenvwrapper-win\n",
    "```\n",
    "\n",
    "##### Step2) 환경변수 설정\n",
    "Mac의 경우\n",
    "- 아래 명령어로 virtualenvwrapper.sh가 설치된 위치 확인(결과값을 기억해둘 것)\n",
    "```\n",
    "$ find / -name virtualenvwrapper.sh 2>/dev/null\n",
    "```\n",
    "> ![Result](snapshot/path.png)\n",
    "> (경로가 두 가지 이상 나오면 둘 중에 하나 선택해서 복사)\n",
    "\n",
    "- 아래 명령어로 .zshrc 파일을 열고 환경 변수를 추가(아래 이미지 마지막 3줄 참고)   \n",
    "```\n",
    "$ vi ~/.zshrc\n",
    "``` \n",
    "> 마지막 줄의 source 뒤의 경로를 위에서 체크한 경로로 변경\n",
    "> ![Result](snapshot/zshrc.png)\n",
    "\n",
    "- 수정한 내용 적용\n",
    "```\n",
    "$ source ~/.zshrc\n",
    "```\n",
    "\n",
    "- 실행 후 재부팅\n",
    "\n",
    "Window의 경우\n",
    "- 아래 명령어로 환경변수 경로를 프로젝트 경로로 설정\n",
    "```\n",
    "$ setx WORKON_HOME=\"프로젝트가 설치된 경로\"  \n",
    "```\n",
    "- 재부팅\n",
    "\n",
    "##### Step3) 가상환경 생성\n",
    "```\n",
    "$ mkvirtualenv test(test 대신 다른 이름 써도 괜찮음)\n",
    "```\n",
    "> ![Result](snapshot/mkvirtualenv.png)\n",
    "\n",
    "> (맥) 이미지 맨 아래에 표시된 부분과 같이 설정한 환경 이름이 표시되면 성공\n",
    "\n",
    "> (윈도우) ls를 터미널에서 입력 후 설정한 환경명이 폴더로 생성되었다면 성공\n",
    "\n",
    "##### Step4) 설치된 가상환경에 프로젝트 실행에 필요한 파이썬 모듈 설치\n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 데이터 학습 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ python train_mask_detector.py --dataset dataset\n",
    "```\n",
    "\n",
    "> dataset 폴더 안에 있는 3800개의 이미지를 가져야 20번의 epoch 동안 각각 96번의 step을 거치며 학습을 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 데이터 학습 결과에 대한 추가 설명"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](snapshot/result-report.png)\n",
    "\n",
    "![Image](plot.png)\n",
    "\n",
    "![Image](snapshot/precision-recall-f1-accuracy.png) \n",
    "\n",
    "- (편의상 TP, TN, FP, FN으로 표시합니다)\n",
    "\n",
    "##### 4-1) Precision\n",
    "> 모델이 True라고 예측한 값 중 실제 결과가 True의 값을 가지는 답의 비율\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{TP}{(TP + FP)}\n",
    "\\end{align}\n",
    "\n",
    "##### 4-2) Recall\n",
    "> 실제 True인 값 중 모델이 True라고 옳게 예측한 답의 비율\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{TP}{(TP + FN)}\n",
    "\\end{align}\n",
    "\n",
    "##### 4-3) F1\n",
    "> Precision과 Recall의 조화평균\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{2 * precision * recall}{(precision + recall)}\n",
    "\\end{align}\n",
    "\n",
    "##### 4-4) Accuracy\n",
    "> True라고 옳게 예측한 것과 False라고 옮게 예측한 것의 비율\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{TP + TN}{(TP + FP + TN + FN)}\n",
    "\\end{align}\n",
    "\n",
    "##### 4-5) 자료 출처\n",
    "> [출처](https://eunsukimme.github.io/ml/2019/10/21/Accuracy-Recall-Precision-F1-score/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) 데이터 검증 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실시간 웹캠 영상을 통한 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ python detect_mask_video.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 보완할 점\n",
    "\n",
    "#### 1) 한정된 데이터\n",
    "- 현재 학습에 사용한 건 3800개의 이미지. 모든 상황을 커버하기는 부족하다. 더 정확한 검증을 위해서는 더 많은 학습 데이터가 필요하다.\n",
    "\n",
    "#### 2) 마스크 불량 착용자 감지\n",
    "- 이 프로젝트는 마스크 착용 여부만 감지한다. 마스크를 코 끝까지 올리지 않은 사람들을 모두 감지하고 싶다면 그 부분에 해당하는 데이터와 레이블이 필요하다. 기존에 학습에 사용한 데이터만큼 많은 수의 데이터가 주어진다면 가능할 것으로 판단된다.\n",
    "\n",
    "#### 3) 관리자에게 알림\n",
    "- 현재는 마스크 불량자를 감지하고 경고를 주는 것에 끝난다. 자동으로 관리자에게 메시지를 보낼 수 있도록 보완하면 좋을 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기대효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 종교시설 등 집단 감염의 위험이 있는 장소에서 마스크 미착용자를 감지하여 건물의 출입이나 이용을 제한하거나 격리 조취를 취할 수 있다. \n",
    "\n",
    "- 다수의 인원이 감지되더라도 실시간으로 마스크 미착용자의 수를 구분해낼 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
